method: bayes
metric:
  goal: maximize
  name: eval/f1
parameters:
  # adam_beta1:
  #   distribution: uniform
  #   max: 1.8
  #   min: 0.45
  # adam_beta2:
  #   distribution: uniform
  #   max: 1.998
  #   min: 0.4995
  # adam_epsilon:
  #   distribution: uniform
  #   max: 2e-08
  #   min: 5e-09
  learning_rate:
    values:
    - 3e-05
  # max_grad_norm:
  #   distribution: uniform
  #   max: 2
  #   min: 0.5
  num_train_epochs_frozen:
    values:
    - 0.0
    - 1.0
    - 2.0
  add_sep_token:
    values:
    - "true"
    # - "false"
  entity_mention:
    values:
    - "entity_start"
    - "max_pooling"
    - "max_pooling_with_markups"
    - "avg_pooling"
    - "avg_pooling_with_markups"
  dropout_rate:
    values:
    - 0.15
    - 0.20
    - 0.25
  label_smoothing_epsilon:
    values:
    - 0.25
    - 0.30
    - 0.35
  fc1_d1_layer_output_size:
    values:
    - 20
    - 30
  fc1_d2_layer_output_size:
    values:
    - 5000
    - 6000
    - 7000
  fc2_layer_output_size:
    values:
    - 5000
    - 6000
    - 7000
  first_layer_d1:
    distribution: int_uniform
    max: 5
    min: 0
  last_layer_d1:
    distribution: int_uniform
    max: 10
    min: 9
  first_layer_d2:
    distribution: int_uniform
    max: 2
    min: 0
  second_to_last_layer_d2:
    distribution: int_uniform
    max: 10
    min: 9
  skip_1_d1:
    distribution: categorical
    values:
    # - "true"
    - "false"
  skip_1_d2:
    distribution: categorical
    values:
    # - "true"
    - "false"
  skip_2_d1:
    distribution: categorical
    values:
    # - "true"
    - "false"
  skip_2_d2:
    distribution: categorical
    values:
    # - "true"
    - "false"
  seed:
    distribution: categorical
    values:
    - 0
    # - 1
    # - 2
    # - 3
program: main.py
command:
  - ${env}
  - ${interpreter}
  - ${program}
  - --disable_tqdm
  - true
  - --do_train
  - --do_eval
  - --output_dir
  - ./output
  - --overwrite_output_dir
  - --per_device_eval_batch_size
  - 16
  - --per_device_train_batch_size
  - 8
  - --evaluation_strategy
  - epoch
  - --model_name_or_path
  - bert-base-uncased
  - --run_name
  - bert-base-uncased
  - --save_steps
  - 0
  - --num_train_epochs
  - 12
  - ${args}